# -*- coding: utf-8 -*-
"""Model of AQI-LSTM-exFactor-Weather-Datasets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ypgyz0UxImar7yLj6InNNVT24bgVZf_D
"""

#https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/05.time-series-forecasting-covid-19.ipynb
#https://www.kaggle.com/code/andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99
#https://cnvrg.io/pytorch-lstm/
#https://towardsdatascience.com/pytorch-lstms-for-time-series-data-cd16190929d7

from google.colab import drive
drive.mount('/content/drive')

#import torch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import rc
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
from torch import nn, optim
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error #as mae
from sklearn.metrics import r2_score
from statistics import mode
#from sklearn.preprocessing import MinMaxScaler
from pandas.plotting import register_matplotlib_converters
import copy
import os
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf # version tensorflow==2.4.0
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
import random
random.seed(2505)
import scipy.stats
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import Conv3D
from keras.layers import ConvLSTM2D
from keras.layers import ConvLSTM3D
from keras.layers import BatchNormalization

#PreProcessing

import os
os.chdir(r"/content/drive/My Drive")

#datasets = pd.read_csv('mpi_roof.csv',encoding='ISO-8859-1')
datasets = pd.read_csv('weather.csv',encoding='ISO-8859-1')
datasets

datasets.drop(['rain (mm)'], inplace=True, axis=1)
datasets.drop(['raining (s)'], inplace=True, axis=1)

#datasets['Date Time'] = pd.to_datetime(datasets['Date Time']).dt.date
datasets['date'] = pd.to_datetime(datasets['date'],format='mixed').dt.date
datasets

#datasets['Date Time']=pd.to_datetime(datasets['Date Time'])

datasets.index = datasets.pop('date')
datasets

#https://www.scribbr.com/statistics/pearson-correlation-coefficient/
import scipy.stats

datasets=datasets.dropna()

corr_mbar, _ = scipy.stats.pearsonr(datasets['OT'],datasets['p (mbar)'])
corr_degC, _ = scipy.stats.pearsonr(datasets['OT'],datasets['T (degC)'])
corr_Tpot, _ = scipy.stats.pearsonr(datasets['OT'],datasets['Tpot (K)'])
corr_Tdew, _ = scipy.stats.pearsonr(datasets['OT'],datasets['Tdew (degC)'])
corr_wd, _ = scipy.stats.pearsonr(datasets['OT'],datasets['wd (deg)'])
corr_rh, _ = scipy.stats.pearsonr(datasets['OT'],datasets['rh (%)'])
corr_VPmax, _ = scipy.stats.pearsonr(datasets['OT'],datasets['VPmax (mbar)'])

print("CO2 (ppm)-mbar",corr_mbar)
print("CO2 (ppm)-degC",corr_degC)
print("CO2 (ppm)-Tpot",corr_Tpot)
print("CO2 (ppm)-Tdew",corr_Tdew)
print("CO2 (ppm)-wd",corr_wd)
print("CO2 (ppm)-rh",corr_rh)
print("CO2 (ppm)-VPmax",corr_VPmax)

if (corr_mbar > corr_degC) and (corr_mbar > corr_Tpot) and (corr_mbar > corr_Tdew) and (corr_mbar > corr_wd) and (corr_mbar > corr_rh) and (corr_mbar > corr_VPmax) :
  ratio = datasets['OT']/((datasets['p (mbar)']))
  ratio =pd.DataFrame(ratio,columns=['Ratio'])
  ex_factor = datasets['p (mbar)']
  ex_factor =pd.DataFrame(ex_factor,columns=['p (mbar)'])
  target = datasets['OT']
  target =pd.DataFrame(target,columns=['OT'])
elif(corr_degC > corr_mbar) and (corr_degC > corr_Tpot) and (corr_degC > corr_Tdew) and (corr_degC > corr_wd) and (corr_degC > corr_rh) and (corr_degC > corr_VPmax):
  ratio = datasets['OT']/((datasets['T (degC)']))
  ratio =pd.DataFrame(ratio,columns=['Ratio'])
  ex_factor = datasets['T (degC)']
  ex_factor =pd.DataFrame(ex_factor,columns=['T (degC)'])
elif(corr_Tpot > corr_mbar) and (corr_Tpot > corr_degC) and (corr_Tpot > corr_Tdew) and (corr_Tpot > corr_wd) and (corr_Tpot > corr_rh) and (corr_Tpot > corr_VPmax):
  ratio = datasets['OT']/((datasets['Tpot (K)']))
  ratio =pd.DataFrame(ratio,columns=['Ratio'])
  ex_factor = datasets['Tpot (K)']
  ex_factor =pd.DataFrame(ex_factor,columns=['Tpot (K)'])
  target = datasets['OT']
  target =pd.DataFrame(target,columns=['OT'])
elif(corr_Tdew > corr_mbar) and (corr_Tdew > corr_degC) and (corr_Tdew > corr_Tpot) and (corr_Tdew > corr_wd) and (corr_Tdew > corr_rh) and (corr_Tdew > corr_VPmax):
  ratio = datasets['OT']/((datasets['Tdew (degC)']))
  ratio =pd.DataFrame(ratio,columns=['Ratio'])
  ex_factor = datasets['Tdew (degC)']
  ex_factor =pd.DataFrame(ex_factor,columns=['Tdew (degC)'])
  target = datasets['OT']
  target =pd.DataFrame(target,columns=['OT'])
elif(corr_wd > corr_mbar) and (corr_wd > corr_degC) and (corr_wd > corr_Tpot) and (corr_wd > corr_Tdew) and (corr_wd > corr_rh) and (corr_wd > corr_VPmax):
  ratio = datasets['OT']/((datasets['wd (deg)']))
  ratio =pd.DataFrame(ratio,columns=['Ratio'])
  ex_factor = datasets['wd (deg)']
  ex_factor =pd.DataFrame(ex_factor,columns=['wd (deg)'])
  target = datasets['OT']
  target =pd.DataFrame(target,columns=['OT'])
elif(corr_rh > corr_mbar) and (corr_rh > corr_degC) and (corr_rh > corr_Tpot) and (corr_rh > corr_Tdew) and (corr_rh > corr_wd) and (corr_rh > corr_VPmax):
  ratio = datasets['OT']/((datasets['rh (%)']))
  ratio =pd.DataFrame(ratio,columns=['Ratio'])
  ex_factor = datasets['rh (%)']
  ex_factor =pd.DataFrame(ex_factor,columns=['rh (%)'])
  target = datasets['OT']
  target =pd.DataFrame(target,columns=['OT'])
else:
  ratio = datasets['OT']/((datasets['VPmax (mbar)']))
  ratio =pd.DataFrame(ratio,columns=['Ratio'])
  ex_factor = datasets['VPmax (mbar)']
  ex_factor =pd.DataFrame(ex_factor,columns=['VPmax (mbar)'])
  target = datasets['OT']
  target =pd.DataFrame(target,columns=['OT'])

target

ratio

ex_factor

## Model Prediction Part

#https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/05.time-series-forecasting-covid-19.ipynb

#ratio = ratio[np.isfinite(ratio).all(1)]

test_data_size = 10540
#test_data_size = 3484

#Target
train_data_target = target[:-test_data_size]
test_data_target = target[-test_data_size:]

#ex_Factor
train_data_ex = ex_factor[:-test_data_size]
test_data_ex = ex_factor[-test_data_size:]

#Ratio
train_data_ratio = ratio[:-test_data_size]
test_data_ratio = ratio[-test_data_size:]

print("Len Train Data Target",train_data_target.shape)
print("Len Train Data exFactor",train_data_ex.shape)
print("Len Train Data Ratio",train_data_ratio.shape)
print("---------------------------------------------")
print("Len Test Data Target",test_data_target.shape)
print("Len Test Data exFactor",test_data_ex.shape)
print("Len Test Data Ratio",test_data_ratio.shape)

scaler = MinMaxScaler(feature_range=(-1, 1))
#df_new = df[np.isfinite(df).all(1)]
#train_data_ratio.replace([np.inf, -np.inf], np.nan, inplace=True)
#test_data_ratio.replace([np.inf, -np.inf], np.nan, inplace=True)
#Target
train_data_target = scaler.fit_transform(train_data_target)
test_data_target = scaler.fit_transform(test_data_target)
#ex_Factor
train_data_ex = scaler.fit_transform(train_data_ex)
test_data_ex = scaler.fit_transform(test_data_ex)
#Ratio
train_data_ratio = scaler.fit_transform(train_data_ratio)
test_data_ratio = scaler.fit_transform(test_data_ratio)

#https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/
def create_sequences(data, seq_length):
    xs = []
    ys = []

    for i in range(len(data)-seq_length-1):
        x = data[i:(i+seq_length)]
        y = data[i+seq_length]
        xs.append(x)
        ys.append(y)

    return np.array(xs), np.array(ys)

seq_length = 24
#Target
X_train_target, y_train_target = create_sequences(train_data_target, seq_length)
X_test_target, y_test_target = create_sequences(test_data_target, seq_length)
X_train_target = torch.from_numpy(X_train_target).float()
y_train_target = torch.from_numpy(y_train_target).float()
X_test_target = torch.from_numpy(X_test_target).float()
y_test_target = torch.from_numpy(y_test_target).float()
#ex_Factor
X_train_ex, y_train_ex = create_sequences(train_data_ex, seq_length)
X_test_ex, y_test_ex = create_sequences(test_data_ex, seq_length)
X_train_ex = torch.from_numpy(X_train_ex).float()
y_train_ex = torch.from_numpy(y_train_ex).float()
X_test_ex = torch.from_numpy(X_test_ex).float()
y_test_ex = torch.from_numpy(y_test_ex).float()
#Ratio
X_train_ratio, y_train_ratio = create_sequences(train_data_ratio, seq_length)
X_test_ratio, y_test_ratio = create_sequences(test_data_ratio, seq_length)
X_train_ratio = torch.from_numpy(X_train_ratio).float()
y_train_ratio = torch.from_numpy(y_train_ratio).float()
X_test_ratio = torch.from_numpy(X_test_ratio).float()
y_test_ratio = torch.from_numpy(y_test_ratio).float()

print("X train target",X_train_target.shape)
print("y train target",y_train_target.shape)
print("-------------------------------------")
print("X train exFactor",X_train_ex.shape)
print("y train exFactor",y_train_ex.shape)
print("-------------------------------------")
print("X train Ratio",X_train_ratio.shape)
print("y train Ratio",y_train_ratio.shape)

print("X test target",X_test_target.shape)
print("y test target",y_test_target.shape)
print("-------------------------------------")
print("X test exFactor",X_test_ex.shape)
print("y test exFactor",y_test_ex.shape)
print("-------------------------------------")
print("X test Ratio",X_test_ratio.shape)
print("y test Ratio",y_test_ratio.shape)

#Target
X_target = np.array(X_train_target)
Y_target = np.array(y_train_target)
#ex_Factor
X_factor = np.array(X_train_ex)
Y_factor = np.array(y_train_ex)
#Ratio
X_ratio = np.array(X_train_ratio)
Y_ratio = np.array(y_train_ratio)

from statistics import mode
import inspect
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error #as mae
from sklearn.metrics import r2_score

class Arwan:
  def __init__(self, epochs, batch_size, learning_rate,beta_1,beta_2):
    self.epochs = epochs
    self.batch_size = batch_size
    self.learning_rate = learning_rate
    self.beta_1 = beta_1
    self.beta_2 = beta_2

  def tensor(self):
    self.test_X_target_cnn = np.array(X_test_target)
    self.test_Y_target_cnn = np.array(y_test_target)
    self.test_Y_target = np.array(y_test_target)
    self.test_X_ex = np.array(X_test_ex)
    self.test_Y_ex = np.array(y_test_ex)
    self.test_X_ratio = np.array(X_test_ratio)
    self.test_Y_ratio = np.array(y_test_ratio)

  def target_model(self):
    #Let's build the ConV LSTM for Target Variable
    model_TargetConvLSTM = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D(filters=32, kernel_size=3,
                               strides=1, padding="causal",
                               activation="relu",
                               #input_shape=(X_train_ratio.shape[1],X_train_ratio.shape[2])),
                               input_shape=(X_train_target.shape[1],X_train_target.shape[2])),

        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8,return_sequences=False)),
        tf.keras.layers.Dense(1),
        tf.keras.layers.Lambda(lambda x: x * 100)]) #original value 200
    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2,amsgrad=False)
    #optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9) #3e-4,1e-5
    model_TargetConvLSTM.compile(loss='mean_squared_error', optimizer = optimizer)
    self.historyTargetConvLSTM = model_TargetConvLSTM.fit(X_target,Y_target,shuffle = False,epochs=self.epochs,batch_size=self.batch_size,validation_split=0.2,verbose=1)
    self.y_pred_target_cnn = model_TargetConvLSTM.predict(self.test_X_target_cnn)[:192] #[:336] #[:720] #[:96]
    #return

  def eFactor_model(self):
    #Let's build Stacked GRU for ExFactor Variable
    model_ex = keras.Sequential()
    #Add a GRU layer with 3 units.
    model_ex.add(layers.GRU(3,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True,
                     input_shape=(X_train_ex.shape[1], X_train_ex.shape[2])))
    model_ex.add(layers.GRU(5,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(8,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(5,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(3,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(1,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=False))
    #Add a dropout layer (penalizing more complex models) -- prevents overfitting
    model_ex.add(layers.Dropout(rate=0.01)) #0.1
    #Add a Dense layer with 1 units (Since we are doing a regression task.
    model_ex.add(layers.Dense(1))
    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2,amsgrad=False)
    #optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9,amsgrad=False) #3e-4,1e-5
    model_ex.compile(loss='mean_squared_error', optimizer = optimizer)
    self.historyEfactor = model_ex.fit(X_factor,Y_factor,shuffle= False,epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2, verbose=1)
    self.y_pred_ex = model_ex.predict(self.test_X_ex)[:192] #[:336] #[:720] #[:96]

  def ratio_model(self):
    #Let's build the BiGRU for Ratio
    model_ratio = keras.Sequential()
    #Add a GRU layer with 3 units.
    model_ratio.add(Bidirectional(layers.GRU(128,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     #input_shape=(X_train_target.shape[1], X_train_target.shape[2]))))
                     input_shape=(X_train_ratio.shape[1], X_train_ratio.shape[2]))))
    model_ratio.add(layers.Dropout(rate=0.01))
    #Add a Dense layer with 1 units (Since we are doing a regression task.
    model_ratio.add(layers.Dense(1))
    #Evaluating loss function of MSE using the adam optimizer.
    #optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99,amsgrad=False) #3e-4,1e-5
    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2,amsgrad=False)
    model_ratio.compile(loss='mean_squared_error', optimizer = optimizer)
    self.historyRatio = model_ratio.fit(X_ratio,Y_ratio, shuffle = False,epochs=self.epochs,batch_size=self.batch_size,validation_split=0.2,verbose=1)
    self.y_pred_ratio = model_ratio.predict(self.test_X_ratio)[:192] #[:336] #[:720] #[:96]
    return

  def train(self):
    target = self.target_model.fit(X_target,Y_target,shuffle = False,epochs=40,batch_size=8,validation_split=0.2,verbose=1)
    #ex = self.eFactor_model.fit(X_factor,Y_factor,shuffle= False,epochs=40, batch_size=8, validation_split=0.2, verbose=1)
    #self.ratio = self.ratio_model.fit(X_ratio,Y_ratio, shuffle = False,epochs=40,batch_size=8,validation_split=0.2,verbose=1)
    #return target, ex, ratio

  def predict(self):
    self.y_pred_target_cnn = self.y_pred_target_cnn.flatten()[:192] #[:336] #[:720] #[:96]
    #y_pred_target = y_pred_target.flatten()
    self.y_pred_ex = self.y_pred_ex.flatten()[:192] #[:336] #[:720] #[:96]
    self.y_pred_ratio = self.y_pred_ratio.flatten()[:192] #[:336] #[:720] #[:96]
    self.y_test_target = self.test_Y_target_cnn.flatten()[:192] #[:336] #[:720] #[:96]
    self.y_test_target = self.y_test_target.flatten()[:192] #[:336] #[:720] #[:96]

    self.mean_cnn_pred = self.y_pred_target_cnn.mean()
    self.median_cnn_pred = np.median(self.y_pred_target_cnn)
    self.mode_cnn_pred = mode(self.y_pred_target_cnn)

    self.mean_cnn_pred = (self.mean_cnn_pred+self.mode_cnn_pred)/2

    if self.mode_cnn_pred > self.mean_cnn_pred:
        self.predictor = self.mode_cnn_pred - self.mean_cnn_pred
    else:
        self.predictor = self.mean_cnn_pred - self.mode_cnn_pred

    self.mean_y_pred_ex = self.y_pred_ex.mean()
    self.mean_y_pred_ratio = self.y_pred_ratio.mean()

    self.AQI_Pred_ratio_cross_mean_ex = self.y_pred_ratio * self.mean_y_pred_ex #Based ratio X exMean
    self.AQI_Pred_ex_cross_mean_ratio = self.y_pred_ex * self.mean_y_pred_ratio

    self.AQI_Pred_ratio_cross_mean_ex_plus_mean_cnn_pred = self.AQI_Pred_ratio_cross_mean_ex + self.mean_cnn_pred #Based ratio X exMean + MeanConvLSTM
    self.AQI_Pred_ex_cross_mean_ratio_plus_mean_cnn_pred = self.AQI_Pred_ex_cross_mean_ratio + self.mean_cnn_pred
    self.AQI_Pred_ex_cross_mean_ratio_plus_mean_cnn_pred_plus_pred = self.AQI_Pred_ratio_cross_mean_ex + self.mean_cnn_pred + self.predictor #Based ratio X exMean + MeanConvLSTM + Predictor

  def dataframe(self):
    self.result = pd.DataFrame(data={'Actual AQI':self.y_test_target,'AQI(ConvLSTM)':self.y_pred_target_cnn,'AQI(R * Ex-Mean)':self.AQI_Pred_ratio_cross_mean_ex,'AQI(R * Ex-Mean + MeanCNN)':self.AQI_Pred_ratio_cross_mean_ex_plus_mean_cnn_pred ,'AQI(R * Ex-Mean + MeanCNN + Predictor)':self.AQI_Pred_ex_cross_mean_ratio_plus_mean_cnn_pred_plus_pred})
    return self.result

  def smape(self,a, f):
    return 1/len(self.a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)

  def matrik_error(self):
    self.mse_ratio_cross_exMean = mean_squared_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean)'])
    self.mae_ratio_cross_exMean = mean_absolute_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean)'])
    #self.smape_ratio_cross_exMean = self.smape(first_strategy_result['Actual AQI'], first_strategy_result['AQI(R * Ex-Mean)'])
    self.r2_ratio_cross_exMean = r2_score(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean)'],multioutput='variance_weighted')
    self.mse_ratio_cross_exMean_plus_meanCNN = mean_squared_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN)'])
    self.mae_ratio_cross_exMean_plus_meanCNN = mean_absolute_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN)'])
    #self.smape_ratio_cross_exMean_plus_meanCNN = self.smape(first_strategy_result['Actual AQI'], first_strategy_result['AQI(R * Ex-Mean + MeanCNN)'])
    self.r2_ratio_cross_exMean_plus_meanCNN = r2_score(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN)'])
    self.r2_ratio_cross_exMean_plus_meanCNN_plus_Predictor = r2_score(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN + Predictor)'],multioutput='variance_weighted')
    self.mse_ratio_cross_exMean_plus_meanCNN_plus_Predictor = mean_squared_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN + Predictor)'])
    self.mae_ratio_cross_exMean_plus_meanCNN_plus_Predictor = mean_absolute_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN + Predictor)'])
    #self.smape_ratio_cross_exMean_plus_meanCNN_plus_Predictor = self.smape(first_strategy_result['Actual AQI'], first_strategy_result['AQI(R * Ex-Mean + MeanCNN + Predictor)'])

  def evaluation(self):
    print("MSE Based ratio X exMean",self.mse_ratio_cross_exMean)
    print("MAE Based ratio X exMean ",self.mae_ratio_cross_exMean)
    #print("Smape Based ratio X exMean ",self.smape_ratio_cross_exMean)
    print("R2 Based ratio X exMean ",self.r2_ratio_cross_exMean)
    print("----------------------------------------------")
    print("MSE Based ratio X exMean + MeanConvLSTM",self.mse_ratio_cross_exMean_plus_meanCNN)
    print("MAE Based ratio X exMean + MeanConvLSTM",self.mae_ratio_cross_exMean_plus_meanCNN)
    #print("Smape Based ratio X exMean + MeanConvLSTM",self.smape_ratio_cross_exMean_plus_meanCNN)
    print("R2 Based ratio X exMean + MeanConvLSTM",self.r2_ratio_cross_exMean_plus_meanCNN)
    print("----------------------------------------------")
    print("MSE Based ratio X exMean + MeanConvLSTM + Predictor",self.mse_ratio_cross_exMean_plus_meanCNN_plus_Predictor)
    print("MAE Based ratio X exMean + MeanConvLSTM + Predictor",self.mae_ratio_cross_exMean_plus_meanCNN_plus_Predictor)
    #print("Smape Based ratio X exMean + MeanConvLSTM + Predictor",self.smape_ratio_cross_exMean_plus_meanCNN_plus_Predictor)
    print("R2 Based ratio X exMean + MeanConvLSTM",self.r2_ratio_cross_exMean_plus_meanCNN_plus_Predictor)

  def train_target_plot(self):
    plt.plot(self.historyTargetConvLSTM.history['loss'], label = 'training loss')
    plt.plot(self.historyTargetConvLSTM.history['val_loss'], label = 'validation loss')
    plt.legend()
    return

  def train_eXfactor_plot(self):
    plt.plot(self.historyEfactor.history['loss'], label = 'training loss')
    plt.plot(self.historyEfactor.history['val_loss'], label = 'validation loss')
    plt.legend()
    return

  def train_ratio_plot(self):
    plt.plot(self.historyRatio.history['loss'], label = 'training loss')
    plt.plot(self.historyRatio.history['val_loss'], label = 'validation loss')
    plt.legend()
    return

  def run_all(self):
    self.tensor()
    self.target_model()
    self.eFactor_model()
    self.ratio_model()
    self.predict()
    self.dataframe()
    self.matrik_error()
    self.evaluation()

  @staticmethod
  def run_all_methods(arg):
    V = Arwan()
    attrs = (getattr(V, name) for name in dir(V))
    methods = filter(inspect.ismethod, attrs)
    result = list()
    for method in methods:
      print(method)
      try:
        df = method(arg)
        result.append(df)
      except Exception as e:
        print(e)
    return pd.concat(result)

#https://medium.com/hackernoon/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b
#https://medium.com/@matteo.arellano/run-all-methods-from-your-class-in-python-c3c93f08932f
#https://pynative.com/python-class-method/
#target = model_TargetConvLSTM.fit(X_target,Y_target,shuffle = False,epochs=400,batch_size=8,validation_split=0.2,verbose=1)
#https://copyprogramming.com/howto/python-do-something-for-any-method-of-a-class
from statistics import mode
import inspect
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error #as mae
from sklearn.metrics import r2_score

class Arwan:
  def __init__(self, epochs, batch_size, learning_rate,beta_1,beta_2):
    self.epochs = epochs
    self.batch_size = batch_size
    self.learning_rate = learning_rate
    self.beta_1 = beta_1
    self.beta_2 = beta_2

  def tensor(self):
    self.test_X_target_cnn = np.array(X_test_target)
    self.test_Y_target_cnn = np.array(y_test_target)
    self.test_Y_target = np.array(y_test_target)
    self.test_X_ex = np.array(X_test_ex)
    self.test_Y_ex = np.array(y_test_ex)
    self.test_X_ratio = np.array(X_test_ratio)
    self.test_Y_ratio = np.array(y_test_ratio)

  def target_model(self):
    #Let's build the ConV LSTM for Target Variable
    model_TargetConvLSTM = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D(filters=32, kernel_size=3,
                               strides=1, padding="causal",
                               activation="relu",
                               #input_shape=(X_train_ratio.shape[1],X_train_ratio.shape[2])),
                               input_shape=(X_train_target.shape[1],X_train_target.shape[2])),

        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8,return_sequences=False)),
        tf.keras.layers.Dense(1),
        tf.keras.layers.Lambda(lambda x: x * 100)]) #original value 200
    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2,amsgrad=False)
    #optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9) #3e-4,1e-5
    model_TargetConvLSTM.compile(loss='mean_squared_error', optimizer = optimizer)
    model_TargetConvLSTM.fit(X_target,Y_target,shuffle = False,epochs=self.epochs,batch_size=self.batch_size,validation_split=0.2,verbose=1)
    self.y_pred_target_cnn = model_TargetConvLSTM.predict(self.test_X_target_cnn)[:96]
    #return

  def eFactor_model(self):
    #Let's build Stacked GRU for ExFactor Variable
    model_ex = keras.Sequential()
    #Add a GRU layer with 3 units.
    model_ex.add(layers.GRU(3,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True,
                     input_shape=(X_train_ex.shape[1], X_train_ex.shape[2])))
    model_ex.add(layers.GRU(5,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(8,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(5,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(3,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=True))
    model_ex.add(layers.GRU(1,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     return_sequences=False))
    #Add a dropout layer (penalizing more complex models) -- prevents overfitting
    model_ex.add(layers.Dropout(rate=0.01)) #0.1
    #Add a Dense layer with 1 units (Since we are doing a regression task.
    model_ex.add(layers.Dense(1))
    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2,amsgrad=False)
    #optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9,amsgrad=False) #3e-4,1e-5
    model_ex.compile(loss='mean_squared_error', optimizer = optimizer)
    model_ex.fit(X_factor,Y_factor,shuffle= False,epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2, verbose=1)
    self.y_pred_ex = model_ex.predict(self.test_X_ex)[:96]
    #return

  def ratio_model(self):
    #Let's build the BiGRU for Ratio
    model_ratio = keras.Sequential()
    #Add a GRU layer with 3 units.
    model_ratio.add(Bidirectional(layers.GRU(128,
                     activation = "tanh",
                     recurrent_activation = "sigmoid",
                     #input_shape=(X_train_target.shape[1], X_train_target.shape[2]))))
                     input_shape=(X_train_ratio.shape[1], X_train_ratio.shape[2]))))
    model_ratio.add(layers.Dropout(rate=0.01))
    #Add a Dense layer with 1 units (Since we are doing a regression task.
    model_ratio.add(layers.Dense(1))
    #Evaluating loss function of MSE using the adam optimizer.
    #optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99,amsgrad=False) #3e-4,1e-5
    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta_1, beta_2=self.beta_2,amsgrad=False)
    model_ratio.compile(loss='mean_squared_error', optimizer = optimizer)
    model_ratio.fit(X_ratio,Y_ratio, shuffle = False,epochs=self.epochs,batch_size=self.batch_size,validation_split=0.2,verbose=1)
    self.y_pred_ratio = model_ratio.predict(self.test_X_ratio)[:96]
    return

  def train(self):
    target = self.target_model.fit(X_target,Y_target,shuffle = False,epochs=40,batch_size=8,validation_split=0.2,verbose=1)
    #ex = self.eFactor_model.fit(X_factor,Y_factor,shuffle= False,epochs=40, batch_size=8, validation_split=0.2, verbose=1)
    #self.ratio = self.ratio_model.fit(X_ratio,Y_ratio, shuffle = False,epochs=40,batch_size=8,validation_split=0.2,verbose=1)
    #return target, ex, ratio

  def predict(self):
    self.y_pred_target_cnn = self.y_pred_target_cnn.flatten() # Predict AQI using ConvLSTM
    #y_pred_target = y_pred_target.flatten()         # predict AQI using Bi-LSTM
    self.y_pred_ex = self.y_pred_ex.flatten()[:96]                 # Predict ExFactor
    self.y_pred_ratio = self.y_pred_ratio.flatten()[:96]
    self.y_test_target = self.test_Y_target_cnn.flatten()[:96]           # Predict Ratio
    self.y_test_target = self.y_test_target.flatten()[:96]

    self.mean_cnn_pred = self.y_pred_target_cnn.mean()
    self.median_cnn_pred = np.median(self.y_pred_target_cnn)
    self.mode_cnn_pred = mode(self.y_pred_target_cnn)

    self.mean_cnn_pred = (self.mean_cnn_pred+self.mode_cnn_pred)/2

    if self.mode_cnn_pred > self.mean_cnn_pred:
        self.predictor = self.mode_cnn_pred - self.mean_cnn_pred
    else:
        self.predictor = self.mean_cnn_pred - self.mode_cnn_pred

    self.mean_y_pred_ex = self.y_pred_ex.mean()
    self.mean_y_pred_ratio = self.y_pred_ratio.mean()

    self.AQI_Pred_ratio_cross_mean_ex = self.y_pred_ratio * self.mean_y_pred_ex # AQI Prediction by Ratio Prediction X Mean ExFactor Prediction
    self.AQI_Pred_ex_cross_mean_ratio = self.y_pred_ex * self.mean_y_pred_ratio # AQI Prediction by Ratio Prediction X Mean ExFactor Prediction

    self.AQI_Pred_ratio_cross_mean_ex_plus_mean_cnn_pred = self.AQI_Pred_ratio_cross_mean_ex + self.mean_cnn_pred
    self.AQI_Pred_ex_cross_mean_ratio_plus_mean_cnn_pred = self.AQI_Pred_ex_cross_mean_ratio + self.mean_cnn_pred
    self.AQI_Pred_ex_cross_mean_ratio_plus_mean_cnn_pred_plus_pred = self.AQI_Pred_ratio_cross_mean_ex + self.mean_cnn_pred + self.predictor

  def dataframe(self):
    self.result = pd.DataFrame(data={'Actual AQI':self.y_test_target,'AQI(ConvLSTM)':self.y_pred_target_cnn,'AQI(R * Ex-Mean)':self.AQI_Pred_ratio_cross_mean_ex,'AQI(R * Ex-Mean + MeanCNN)':self.AQI_Pred_ratio_cross_mean_ex_plus_mean_cnn_pred ,'AQI(R * Ex-Mean + MeanCNN + Predictor)':self.AQI_Pred_ex_cross_mean_ratio_plus_mean_cnn_pred_plus_pred})
    return self.result

  def smape(self,a, f):
    return 1/len(self.a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)

  def matrik_error(self):
    self.mse_only_ConvLSTM = mean_squared_error(self.result['Actual AQI'], self.result['AQI(ConvLSTM)'])
    self.mae_only_ConvLSTM = mean_absolute_error(self.result['Actual AQI'], self.result['AQI(ConvLSTM)'])
    #self.smape_only_ConvLSTM = self.smape(first_strategy_result['Actual AQI'], first_strategy_result['AQI(ConvLSTM)'])
    self.r2_only_ConvLSTM = r2_score(self.result['Actual AQI'], self.result['AQI(ConvLSTM)'],multioutput='variance_weighted')
    self.mse_ratio_cross_exMean = mean_squared_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean)'])
    self.mae_ratio_cross_exMean = mean_absolute_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean)'])
    #self.smape_ratio_cross_exMean = self.smape(first_strategy_result['Actual AQI'], first_strategy_result['AQI(R * Ex-Mean)'])
    self.r2_ratio_cross_exMean = r2_score(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean)'],multioutput='variance_weighted')
    self.mse_ratio_cross_exMean_plus_meanCNN = mean_squared_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN)'])
    self.mae_ratio_cross_exMean_plus_meanCNN = mean_absolute_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN)'])
    #self.smape_ratio_cross_exMean_plus_meanCNN = self.smape(first_strategy_result['Actual AQI'], first_strategy_result['AQI(R * Ex-Mean + MeanCNN)'])
    self.r2_ratio_cross_exMean_plus_meanCNN = r2_score(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN)'])
    self.r2_ratio_cross_exMean_plus_meanCNN_plus_Predictor = r2_score(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN + Predictor)'],multioutput='variance_weighted')
    self.mse_ratio_cross_exMean_plus_meanCNN_plus_Predictor = mean_squared_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN + Predictor)'])
    self.mae_ratio_cross_exMean_plus_meanCNN_plus_Predictor = mean_absolute_error(self.result['Actual AQI'], self.result['AQI(R * Ex-Mean + MeanCNN + Predictor)'])
    #self.smape_ratio_cross_exMean_plus_meanCNN_plus_Predictor = self.smape(first_strategy_result['Actual AQI'], first_strategy_result['AQI(R * Ex-Mean + MeanCNN + Predictor)'])

  def evaluation(self):
    #print("MSE only_ConvLSTM",self.mse_only_ConvLSTM)
    #print("MAE only_ConvLSTM",self.mae_only_ConvLSTM)
    #print("Smape only_ConvLSTM",self.smape_only_ConvLSTM)
    #print("R2 only_ConvLSTM",self.r2_only_ConvLSTM)
    #print("----------------------------------------------")
    #print("MSE Based ratio X exMean",self.mse_ratio_cross_exMean)
    #print("MAE Based ratio X exMean ",self.mae_ratio_cross_exMean)
    #print("Smape Based ratio X exMean ",self.smape_ratio_cross_exMean)
    #print("R2 Based ratio X exMean ",self.r2_ratio_cross_exMean)
    print("----------------------------------------------")
    print("MSE Based ratio X exMean + MeanConvLSTM",self.mse_ratio_cross_exMean_plus_meanCNN)
    print("MAE Based ratio X exMean + MeanConvLSTM",self.mae_ratio_cross_exMean_plus_meanCNN)
    #print("Smape Based ratio X exMean + MeanConvLSTM",self.smape_ratio_cross_exMean_plus_meanCNN)
    print("R2 Based ratio X exMean + MeanConvLSTM",self.r2_ratio_cross_exMean_plus_meanCNN)
    print("----------------------------------------------")
    #print("MSE Based ratio X exMean + MeanConvLSTM + Predictor",self.mse_ratio_cross_exMean_plus_meanCNN_plus_Predictor)
    #print("MAE Based ratio X exMean + MeanConvLSTM + Predictor",self.mae_ratio_cross_exMean_plus_meanCNN_plus_Predictor)
    #print("Smape Based ratio X exMean + MeanConvLSTM + Predictor",self.smape_ratio_cross_exMean_plus_meanCNN_plus_Predictor)
    #print("R2 Based ratio X exMean + MeanConvLSTM",self.r2_ratio_cross_exMean_plus_meanCNN_plus_Predictor)

  def run_all(self):
    self.tensor()
    self.target_model()
    self.eFactor_model()
    self.ratio_model()
    self.predict()
    self.dataframe()
    #arwan.smape()
    self.matrik_error()
    self.evaluation()


  @staticmethod
  def run_all_methods(arg):
    V = Arwan()
    attrs = (getattr(V, name) for name in dir(V))
    methods = filter(inspect.ismethod, attrs)
    result = list()
    for method in methods:
      print(method)
      try:
        df = method(arg)
        result.append(df)
      except Exception as e:
        print(e)
    return pd.concat(result)

#(self, epochs, batch_size, learning_rate,beta_1,beta_2)
arwan = Arwan(12,8,0.001,0.9,0.99)
arwan.run_all()
#arwan.tensor()
#arwan.target_model()
#arwan.eFactor_model()
#arwan.ratio_model()
#arwan.predict()
#arwan.dataframe()
#arwan.smape()
#arwan.matrik_error()
#arwan.evaluation()
#arwan.train()
#arwan
#arwan.train